# -*- coding: utf-8 -*-
"""
Created on Wed Jun 20 09:40:50 2018

@author: Allen Prasad
"""
#importing Libraries
import pandas as pd
from sklearn.cross_validation import train_test_split
import numpy as np
import sklearn.preprocessing as preprocessing
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
sns.set()

#importing data
data = pd.read_csv("D:/New folder (3)/BikeBuyerWithLocation (3).csv", encoding = "ISO-8859-1")

#Exploratory data analysis

data.head()

data.shape

data.info()

data.isnull().sum()

data.describe()

#Function for analyzing target with individual feature
def bar_chart(feature):
    Buyer = data[data['BikeBuyer']==1][feature].value_counts()
    notabuyer = data[data['BikeBuyer']==0][feature].value_counts()
    df = pd.DataFrame([Buyer,notabuyer])
    df.index = ['buyer','notabuyer']
    df.plot(kind='bar', figsize=(10,5),label = True)

    
#Function for encoding categorical variable into numbers    
def labelencoder(feature = []):
    labelencoder = preprocessing.LabelEncoder()
    print('labelencoding started')
    for i in feature:
        data[i] = labelencoder.fit_transform(data[i].astype('str'))
    print('labelencoding completed successfully') 

#Encoding Target variable for further analysis
labelencoder(['BikeBuyer'])
   

#Exploratory data analysis and feature engineering

# 1. Marital Status 
 
data['Marital Status'].value_counts()

#Married    5673
#Single     4327

#encoding
labelencoder(['Marital Status'])   
    
data['Marital Status'].value_counts()

#0    5673
#1    4327

#ploting the data for analysis
bar_chart('Marital Status')

# 2. Gender

data['Gender'].value_counts()

#Male      5113
#Female    4887

#encoding
labelencoder(['Gender'])   
    
data['Gender'].value_counts()
#1    5113
#0    4887

#ploting the data for analysis
bar_chart('Gender')


#3.'Education', 

data['Education'].value_counts()
#Partial College        2698
#Bachelors              2578
#High School            1979
#Graduate Degree        1635
#Partial High School    1110

labelencoder(['Education']) 
  
    
data['Education'].value_counts()
#3    2698
#0    2578
#2    1979
#1    1635
#4    1110

#ploting the data for analysis
bar_chart('Education')

#4.'Occupation', 

data['Occupation'].value_counts()

#Professional      2953
#Skilled Manual    2495
#Management        1730
#Clerical          1423
#Manual            1399

labelencoder(['Occupation'])   
    
data['Occupation'].value_counts()
#3    2953
#4    2495
#1    1730
#0    1423
#2    1399

#ploting the data for analysis
bar_chart('Occupation')

#5.'Home Owner', 

data['Home Owner'].value_counts()
#Yes    6779
#No     3221

labelencoder(['Home Owner'])   
    
data['Home Owner'].value_counts()
#1    6779
#0    3221

#ploting the data for analysis
bar_chart('Home Owner')

#5.'Commute Distance', 

data['Commute Distance'].value_counts()
#0-1 Miles     3081
#5-10 Miles    1959
#1-2 Miles     1810
#10+ Miles     1630
#2-5 Miles     1520

labelencoder(['Commute Distance'])   
    
data['Commute Distance'].value_counts()
#0    3081
#4    1959
#1    1810
#2    1630
#3    1520

#ploting the data for analysis
bar_chart('Commute Distance')

#5.'Region', 

data['Region'].value_counts()
#North America    5456
#Europe           2928
#Pacific          1616

labelencoder(['Region'])   
    
data['Region'].value_counts()
#1    5456
#0    2928
#2    1616

#ploting the data for analysis
bar_chart('Region')

##############################

#Analysis of Yearly income by ploting data

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Yearly Income',shade= True)
facet.set(xlim=(0, data['Yearly Income'].max()))
facet.add_legend()
 
plt.show()

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Yearly Income',shade= True)
facet.set(xlim=(0, data['Yearly Income'].max()))
facet.add_legend()
plt.xlim(35000,80000)


#Binning the yearly income data to convert continous into categorical variable
data.loc[ data['Yearly Income'] <= 40000, 'Yearly Income'] = 0
data.loc[(data['Yearly Income'] > 40000) & (data['Yearly Income'] <= 80000), 'Yearly Income'] = 1
data.loc[(data['Yearly Income'] > 80000) & (data['Yearly Income'] <= 120000), 'Yearly Income'] = 2
data.loc[(data['Yearly Income'] > 120000) & (data['Yearly Income'] <= 170000), 'Yearly Income'] = 3


######################

#Analysis of Age by ploting data

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, data['Age'].max()))
facet.add_legend()
 
plt.show()

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, data['Age'].max()))
facet.add_legend()
plt.xlim(20,30)

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, data['Age'].max()))
facet.add_legend()
plt.xlim(30,40)

facet = sns.FacetGrid(data, hue="BikeBuyer",aspect=4)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, data['Age'].max()))
facet.add_legend()
plt.xlim(50,60)


#Binning the age data to convert continous into categorical variable

data.loc[ data['Age'] <= 30, 'Age'] = 0
data.loc[(data['Age'] > 30) & (data['Age'] <= 50), 'Age'] = 1
data.loc[(data['Age'] > 50) & (data['Age'] <= 70), 'Age'] = 2
data.loc[(data['Age'] > 70) & (data['Age'] <= 100), 'Age'] = 3


##############

#imputing null values of Latitude and Longitude by grouping commute distance and divided by 10 to reduce the scale

data["Latitude"].fillna(data.groupby("Commute Distance")["Latitude"].transform("median"), inplace=True)

data["Longitude"].fillna(data.groupby("Commute Distance")["Longitude"].transform("median"), inplace=True)

data["Latitude"] = data["Latitude"]/10

data["Longitude"]  = data["Longitude"]/10

################

#handling Country, zip code and city data by encoding and imputing 

#Country
    
data['Country'].value_counts()

data['Country'] = data['Country'].fillna(value = 0)

labelencoder(['Country'])   
    
data['Country'].value_counts()

bar_chart('Country')


#zip code

data['Zip Code'].value_counts()

data['Zip Code'] = data['Zip Code'].fillna(value = 0)

labelencoder(['Zip Code'])   
    
data['Zip Code'].value_counts()

#city
    
data['City'].value_counts()

data['City'] = data['City'].fillna(value = 0)

labelencoder(['City'])   
    
data['City'].value_counts()
  
####################

#dividing zip code and city by 100 to reduce the scale

data["Zip Code"] = data["Zip Code"]/100

data["City"] = data["City"]/100

data["Zip Code"].describe()

data["City"].describe()



# Importing Classifier Modules# Import 
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.tree import DecisionTreeClassifier
import numpy as np

data_new = data.drop('BikeBuyer', axis=1)

target = data['BikeBuyer']

data.shape
#(10000, 18)

data_new.shape
#(10000, 17)

target.shape
#(10000,)

from sklearn.cross_validation import train_test_split


#spliting test train data
X_train, X_test, y_train, y_test = train_test_split(data_new, target, random_state=1)

X_train = X_train.drop('ID', axis=1)

X_test_new = X_test.drop('ID', axis=1)

#training the models

##############################

# Logistic Regression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
acc_log = round(logreg.score(X_test_new, y_test) * 100, 2)
acc_log

coeff_df = pd.DataFrame(X_train.columns.delete(0))
coeff_df.columns = ['Feature']
coeff_df["Correlation"] = pd.Series(logreg.coef_[0])

coeff_df.sort_values(by='Correlation', ascending=False)


# Support Vector Machines

svc = SVC()
svc.fit(X_train, y_train)
acc_svc = round(svc.score(X_test_new, y_test) * 100, 2)
acc_svc

#KNN

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, y_train)
acc_knn = round(knn.score(X_test_new, y_test) * 100, 2)

# Perceptron

perceptron = Perceptron()
perceptron.fit(X_train, y_train)
acc_perceptron = round(perceptron.score(X_test_new, y_test) * 100, 2)
acc_perceptron


#decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
acc_decision_tree = round(decision_tree.score(X_test_new, y_test) * 100, 2)
acc_decision_tree


######################

#Arranging the score

Final_score = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
                'Perceptron',
              'Decision Tree'],
    'Score': [acc_svc, acc_knn, acc_log, 
               acc_perceptron, acc_decision_tree]})
    
Final_score.sort_values(by='Score', ascending=False)


#Predicting the test data using the highly scored logistic regression
Y_pred = logreg.predict(X_test_new)

final = pd.DataFrame({
        "ID": X_test["ID"],
        "BikeBuyer": Y_pred
    })
    
#########################################################################################







